{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open('../names.txt','r').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stoi and itos dicts\n",
    "chars = ['.']+sorted([chr(char) for char in range(ord('a'),ord('z')+1)])+['<s>']\n",
    "stoi = {s:i for i,s in enumerate(chars)}\n",
    "itos = {i:s for s,i in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = torch.zeros(28,28,27) # cuz I wouldn't expect <s> for the 3rd dimension. and anyway its just a filler character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filling the N array.\n",
    "for word in words:\n",
    "    chs = ['.']+['<s>']+list(word)+['.']\n",
    "    for cha,chb,chc in zip(chs,chs[1:],chs[2:]):\n",
    "        # print(f'cha,chb,chc : {cha,chb,chc}')\n",
    "        i,j,k = stoi[cha],stoi[chb],stoi[chc]\n",
    "        N[i,j,k]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N.sum() # the count will be same as you can imagine each increment is like sliding over the name and turns out with your current approach you are sliding as much as when you were doing bigrams with just window size of 3.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = (N+1).float()\n",
    "# print(P.sum(dim=2).shape) #wrong # torch.Size([28, 28])\n",
    "# print(P.shape) # torch.Size([28, 28, 27])\n",
    "# P.sum(dim=2,keepdim=True).shape #torch.Size([28, 28, 1])\n",
    "P/=P.sum(dim=2,keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_likelihood=0.0\n",
    "ncount=0\n",
    "for w in words:\n",
    "    # chs = ['.']+['<s>']+list(word)+['.']\n",
    "    chs = ['.']+['<s>']+list(w)+['.'] #bug identified.\n",
    "    for cha,chb,chc in zip(chs,chs[1:],chs[2:]):\n",
    "        i,j,k = stoi[cha],stoi[chb],stoi[chc]\n",
    "        prob = P[i,j,k]\n",
    "        log_likelihood+=torch.log(prob)\n",
    "        ncount+=1\n",
    "neg_log_likelihood = -log_likelihood\n",
    "mean_neg_log_likelihood = neg_log_likelihood/ncount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{neg_log_likelihood=}')\n",
    "print(f'{mean_neg_log_likelihood=}')\n",
    "print(ncount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_neg_log_likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "\n",
    "for _ in range(10):\n",
    "    nameList = []\n",
    "    ix,iy =0,27\n",
    "    while True:\n",
    "        p = P[ix,iy]\n",
    "        iz = torch.multinomial(p,num_samples=1,replacement=True,generator=g).item()\n",
    "        if iz==0:\n",
    "            break\n",
    "        nameList.append(itos[iz])\n",
    "        ix,iy=iy,iz\n",
    "    print(''.join(nameList))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing the neural net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnNames = list(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.shuffle(nnNames)\n",
    "\n",
    "train_size = int(0.8 * len(nnNames))\n",
    "dev_size = int(0.1 * len(nnNames))\n",
    "test_size = len(nnNames) - train_size - dev_size\n",
    "\n",
    "train_data = nnNames[:train_size]\n",
    "dev_data = nnNames[train_size:train_size + dev_size]\n",
    "test_data = nnNames[train_size + dev_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs,ys,zs = [],[],[]\n",
    "\n",
    "# for name in nnNames: # should be using train data here\n",
    "for name in train_data:\n",
    "    # n = ['.']+['<s>']+list(word)+['.'] #same bug using word instead of name...\n",
    "    n = ['.']+['<s>']+list(name)+['.']\n",
    "    for cha,chb,chc in zip(n,n[1:],n[2:]):\n",
    "        xs.append(stoi[cha])\n",
    "        ys.append(stoi[chb])\n",
    "        zs.append(stoi[chc])\n",
    "\n",
    "xs,ys,zs = torch.tensor(xs),torch.tensor(ys),torch.tensor(zs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs.shape,ys.shape,zs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# was thinking like how we used W as the same dimension as N in bigram I think I should do the same here\n",
    "W = torch.randn((28,28,27),requires_grad=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpyW = W.clone().detach()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zenc = F.one_hot(zs,num_classes=27).float()\n",
    "zenc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W[1,1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W[[1,2],[1,2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_size = 0.01*5000\n",
    "for i in range(10):\n",
    "    #forward pass\n",
    "    logits  =W[xs,ys,:]\n",
    "    loss = F.cross_entropy(logits,zenc)+ 0*(W**2).mean() #p_loss + smoothing factor\n",
    "\n",
    "    #backward pass\n",
    "    W.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    #update \n",
    "    W.data+=W.grad*step_size*-1\n",
    "\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to get the loss closer to the theoretical limit (one obtained by the counting method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backpropagation to get the loss value of the nn method as close as counting method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_size = 0.01*1\n",
    "d_loss = torch.tensor(2.2120)\n",
    "for i in range(10):\n",
    "    #forward pass\n",
    "    logits  =W[xs,ys,:]\n",
    "    p_loss = F.cross_entropy(logits,zenc)+ 0*(W**2).mean() #p_loss + smoothing factor\n",
    "    loss = torch.abs(p_loss-d_loss)\n",
    "\n",
    "    #backward pass\n",
    "    W.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    #update \n",
    "    W.data+=W.grad*step_size*-1\n",
    "\n",
    "    print(loss)\n",
    "print(p_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sampling\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "for i in range(10):\n",
    "    out = []\n",
    "    ix,iy= 0,27 # ix = . iy = <s>\n",
    "    while True:\n",
    "        l_logits = W[ix,iy] # gives the 3rd dimension of the weight corresponding to each 27 neurons of the pair ix,iy\n",
    "        l_counts = l_logits.exp()\n",
    "        l_p = l_counts/l_counts.sum()\n",
    "\n",
    "        iz = torch.multinomial(l_p,num_samples=1,replacement=True,generator=g).item()\n",
    "        if iz==0:\n",
    "            break\n",
    "        out.append(itos[iz])\n",
    "        ix,iy = iy,iz\n",
    "    print(''.join(out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay so its a little better now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turns out ther was a bug in the loop that prepared the xs,ys and zs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing of the model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx,ty,tz = [],[],[]\n",
    "for name in test_data:\n",
    "    n = ['.']+['<s>']+list(name)+['.']\n",
    "    for cha,chb,chc in zip(n,n[1:],n[2:]):\n",
    "        tx.append(stoi[cha])\n",
    "        ty.append(stoi[chb])\n",
    "        tz.append(stoi[chc])\n",
    "\n",
    "tx,ty,tz = torch.tensor(tx),torch.tensor(ty),torch.tensor(tz)\n",
    "tzenc = F.one_hot(tz,num_classes=27).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx.shape,ty.shape,tz.shape,tzenc.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_logits = W[tx,ty]\n",
    "loss = F.cross_entropy(t_logits,tzenc)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_23756\\3941473048.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  W = torch.load('trigram_nn_model.pth')\n"
     ]
    }
   ],
   "source": [
    "# torch.save(W, 'trigram_nn_model.pth') dont call it unnecessarily it can override things.\n",
    "W = torch.load('trigram_nn_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing the model smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([22879]),\n",
       " torch.Size([22879]),\n",
       " torch.Size([22879]),\n",
       " torch.Size([22879, 27]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dx,dy,dz = [],[],[]\n",
    "for name in dev_data:\n",
    "    n = ['.']+['<s>']+list(name)+['.']\n",
    "    for cha,chb,chc in zip(n,n[1:],n[2:]):\n",
    "        dx.append(stoi[cha])\n",
    "        dy.append(stoi[chb])\n",
    "        dz.append(stoi[chc])\n",
    "    \n",
    "dx,dy,dz = torch.tensor(dx),torch.tensor(dy),torch.tensor(dz)\n",
    "dzenc = F.one_hot(dz,num_classes=27).float()\n",
    "dx.shape,dy.shape,dz.shape,dzenc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.2770, grad_fn=<SubBackward0>)\n",
      "tensor(2.2770, grad_fn=<SubBackward0>)\n",
      "tensor(2.2770, grad_fn=<SubBackward0>)\n",
      "tensor(2.2770, grad_fn=<SubBackward0>)\n",
      "tensor(2.2770, grad_fn=<SubBackward0>)\n",
      "tensor(2.2770, grad_fn=<SubBackward0>)\n",
      "tensor(2.2769, grad_fn=<SubBackward0>)\n",
      "tensor(2.2769, grad_fn=<SubBackward0>)\n",
      "tensor(2.2769, grad_fn=<SubBackward0>)\n",
      "tensor(2.2769, grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "step_size = 0.01*5\n",
    "smoothing_factor=0\n",
    "for i in range(10):\n",
    "    #forward pass\n",
    "    logits  =W[dx,dy,:]\n",
    "    loss = F.cross_entropy(logits,dzenc)-smoothing_factor*(W**2).mean()\n",
    "\n",
    "    #backward pass\n",
    "    W.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    #update \n",
    "    W.data+=W.grad*step_size*-1\n",
    "\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating the loss over dev data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.2786, grad_fn=<DivBackward1>)"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_logits = W[dx,dy]\n",
    "loss = F.cross_entropy(d_logits,dzenc)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sampling from dev data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "junidedianaqad\n",
      "paufby\n",
      "adin\n",
      "kai\n",
      "ritoper\n",
      "sathem\n",
      "dahnaaurinileviassdbduinrwibba\n",
      "sejyiely\n",
      "arte\n",
      "faveumtsyfoltumj\n"
     ]
    }
   ],
   "source": [
    "#sampling\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "for i in range(10):\n",
    "    out = []\n",
    "    ix,iy= 0,27 # ix = . iy = <s>\n",
    "    while True:\n",
    "        d_logits = W[ix,iy] # gives the 3rd dimension of the weight corresponding to each 27 neurons of the pair ix,iy\n",
    "        d_counts = d_logits.exp()\n",
    "        d_p = d_counts/d_counts.sum()\n",
    "\n",
    "        iz = torch.multinomial(d_p,num_samples=1,replacement=True,generator=g).item()\n",
    "        if iz==0:\n",
    "            break\n",
    "        out.append(itos[iz])\n",
    "        ix,iy = iy,iz\n",
    "    print(''.join(out))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (virtual micrograd)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
